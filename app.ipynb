{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet gradio langchain_community langchain langchain_groq langchain_huggingface faiss-gpu faiss-cpu bitsandbytes torch transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "import base64\n",
    "import time\n",
    "import torch\n",
    "import traceback\n",
    "import random\n",
    "import gradio as gr\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from getpass import getpass\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.callbacks.uptrain_callback import UpTrainCallbackHandler\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-m3\", model_kwargs=model_kwargs, \n",
    ")\n",
    "# hf = HuggingFaceEmbeddings(model_name=\"thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load llm from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, token=\"## hf token\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=hf_model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=400,\n",
    ")\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLama via groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get free key from https://console.groq.com/keys\n",
    "llm = ChatGroq(temperature=1, groq_api_key=\"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\",\n",
    "               model_name=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and create embeddings from nsmq past questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"faiss_index\"):\n",
    "    print(\"Loading.....\")\n",
    "    vectorstore = FAISS.load_local(\n",
    "        \"faiss_index\", hf, allow_dangerous_deserialization=True)\n",
    "    print(\"Loaded\")\n",
    "else:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=100)\n",
    "    loader = TextLoader(file_path=\"assistant/formatted_questions_1.txt\", encoding='utf-8')\n",
    "    data = loader.load()\n",
    "    splits = text_splitter.split_documents(data)\n",
    "    print(\"Splitted\")\n",
    "    print(\"Vectoring...\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=splits, embedding=hf)\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "    print(\"Vectored and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prompts and retriever for rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "## Answer question ###\n",
    "system_prompt = (\n",
    "    \"\"\"\n",
    "You are NSMQ-Assistant, a specialized tutor dedicated to helping students excel in the National Science & Maths Quiz (NSMQ). Your role is to provide personalized, adaptive support tailored to each student's unique strengths and areas for improvement. Utilize the following retrieved context, including past questions from the years 2009, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, and 2021, to deliver accurate, succinct, and insightful responses.\n",
    "\n",
    "When asked for a past question, use retrieve the appropriate one.\n",
    "Quiz Structure\n",
    "Three schools compete in each contest and each school is represented by two contestants. The current quiz mistress is Dr. Elsie Effah Kaufmann. Presently, every contest is composed of five rounds with the following rules:\n",
    "Round 1 — The round of fundamental questions. Each contesting school has to answer 4 Biology, 4 Chemistry, 4 Physics and 4 Mathematics questions. A wrongly answered question may be carried over as a bonus. Partial credit is sometimes awarded by the quiz mistress.\n",
    "Round 2 — Called the speed race. All three schools are presented with the same mainly applied questions at the same time. A school answers a question by ringing the bell. There are no partial credits at this stage and a school gains a maximum of three points for answering a question correctly.\n",
    "Round 3 — known as the Problem of the Day. The contestants are required to solve a single question, worth 10 points, within 4 minutes.\n",
    "Round 4 — True or False statements are given to the contestants in turns. The objective is to determine whether each statement is true or false. A correctly answered question fetches 2 points. A wrongly answered question attracts a penalty of -1 point. One may decide not to answer a question, in which case it will be carried over to the next contesting school as a bonus for the full benefit of the two points.\n",
    "Round 5 — Riddles; clues are given one after the other to the contesting schools. Getting the correct answer on the first clue fetches 5 points. On the second clue, 4 points are awarded. On the third or any other subsequent clue 3 points it given. There are 4 riddles in all.\n",
    "        eg. the first clue may be 'I come in all shapes and forms.' so give this and wait for a response, if wrong proceed and add the next clue and so on\n",
    "\n",
    "If you are asked simulate the Quiz mistress, check out these descriptions above. Always question and score the student like he was in an actually quiz. Give the instructions and follow as due. Always retrieve questions unless the user asks for a question outside your knowledge.\n",
    "REMEMBER: ALWAYS RETRIEVE FROM YOUR CONTEXT UNLESS USER ASKS OTHERWISE\n",
    "\n",
    "Guidelines:\n",
    "Personalization: Adapt your responses to match the student's current experience with the quiz, knowledge level, strengths and weakness, learning style, and pace. Offer more detail for beginners and concise, advanced explanations for proficient students.\n",
    "For a beginner, teach them the quiz structure into details.\n",
    "\n",
    "Authority: Answer confidently as if you possess inherent knowledge, incorporating past quiz content and any relevant data seamlessly.\n",
    "\n",
    "Direct Communication: Avoid references to the context or retrieval process. Speak directly and clearly, maintaining a conversational tone that is easy to understand.\n",
    "\n",
    "Conciseness: Keep your answers brief, within three sentences, while ensuring completeness. Strive for clarity and relevance in every response.\n",
    "\n",
    "Constructive Feedback: When a student makes a mistake or has gaps in their knowledge, provide corrective guidance with supportive feedback to encourage improvement.\n",
    "\n",
    "Limitations: If the information is not available or beyond your scope, clearly state, “I don't know the answer to that.”\n",
    "\n",
    "Additional Resources: Where appropriate, suggest specific topics, past papers, or study strategies that could benefit the student's preparation further.\n",
    "\n",
    "    {context}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(\n",
    "    history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    print(\"session-----------------\", store[session_id])\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n",
    "\n",
    "# def predict(message, history):\n",
    "#     # Initialize the conversation with a system prompt if history is empty\n",
    "#     if not history:\n",
    "#         system_prompt = \"You are Sterling GPT a helpful assistant? You are always concise and give short in you answers unless the user wants a longer answer. You a truthful with a good sense of humor\"\n",
    "#         # System message with no corresponding AI response yet\n",
    "#         history = [(system_prompt, \"\")]\n",
    "\n",
    "#     history_langchain_format = []\n",
    "#     for human, ai in history:\n",
    "#         history_langchain_format.append(HumanMessage(content=human))\n",
    "#         history_langchain_format.append(AIMessage(content=ai))\n",
    "#     history_langchain_format.append(HumanMessage(content=message))\n",
    "#     gpt_response = llm(history_langchain_format)\n",
    "#     print(history_langchain_format)\n",
    "#     return gpt_response.content\n",
    "\n",
    "\n",
    "session_id = str(random.randint(1, 200))\n",
    "\n",
    "rag_history = []\n",
    "def rag(message, history):\n",
    "    response = rag_chain.invoke({\"input\": message, \"chat_history\": rag_history})\n",
    "    rag_history.extend(\n",
    "        [\n",
    "            HumanMessage(content=message),\n",
    "            AIMessage(content=response[\"answer\"]),\n",
    "        ]\n",
    "    )\n",
    "    # answer = response['answer']\n",
    "    # print(rag_history)\n",
    "    return response[\"answer\"]\n",
    "\n",
    "\n",
    "def predict_rag(message, history):\n",
    "    history.append((message, \"\"))\n",
    "\n",
    "    # Convert history to the required format for the conversational model\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        if ai:\n",
    "            history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    response = conversational_rag_chain.stream({\"input\": message},\n",
    "                                               config={\n",
    "        \"configurable\": {\"session_id\": session_id}\n",
    "    },)\n",
    "\n",
    "    partial_response = \"\"\n",
    "    for chunk in response:\n",
    "        if answer_chunk := chunk.get(\"answer\"):\n",
    "            partial_response += answer_chunk\n",
    "            # say(answer_chunk)\n",
    "            print(f\"{answer_chunk}\", end=\"\")\n",
    "            yield partial_response\n",
    "\n",
    "    # Update the last entry with the AI's response\n",
    "    history[-1] = (message, response)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below runs only the text model, skip this cell to proceed to the full app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    "#chatbot {\n",
    "    flex-grow: 2 !important;  /* Changed size by increasing flex-grow */\n",
    "    overflow: auto !important;\n",
    "}\n",
    "#col { height: calc(90vh - 2px - 16px) !important; }\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    # gr.Markdown(\"## Chat with me\")\n",
    "    with gr.Tab(\"Prep\", ):\n",
    "        # chatbot = gr.Chatbot()\n",
    "        # msg = gr.Textbox()\n",
    "        # clear = gr.ClearButton([msg, chatbot])\n",
    "        # msg.submit(predict, [msg, chatbot], [msg, chatbot])\n",
    "        with gr.Row():\n",
    "            # with gr.Column(scale=3):\n",
    "            #     response_limit = gr.Number(label=\"Response Limit\", value=10, interactive=True)\n",
    "            with gr.Column(scale=1,elem_id=\"col\"):\n",
    "                chat = gr.ChatInterface(\n",
    "                    fn=rag,\n",
    "                    chatbot=gr.Chatbot(elem_id=\"chatbot\",\n",
    "                                    render=False),\n",
    "\n",
    "                    \n",
    "                    # additional_inputs=[response_limit]\n",
    "                )\n",
    "           \n",
    "    with gr.Tab(\"Fast QA\"):\n",
    "        with gr.Column(elem_id=\"col\"):\n",
    "            chat = gr.ChatInterface(predict_rag, fill_height=True)\n",
    "            \n",
    "demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow to use Vision Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True, torch_dtype=torch.float16)\n",
    "model = model.to(device='cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-Llama3-V-2_5', trust_remote_code=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "ERROR_MSG = \"Error, please retry\"\n",
    "model_name = 'MiniCPM-Llama3-V 2.5'\n",
    "\n",
    "def create_component(params, comp='Slider'):\n",
    "    if comp == 'Slider':\n",
    "        return gr.Slider(\n",
    "            minimum=params['minimum'],\n",
    "            maximum=params['maximum'],\n",
    "            value=params['value'],\n",
    "            step=params['step'],\n",
    "            interactive=params['interactive'],\n",
    "            label=params['label']\n",
    "        )\n",
    "    elif comp == 'Radio':\n",
    "        return gr.Radio(\n",
    "            choices=params['choices'],\n",
    "            value=params['value'],\n",
    "            interactive=params['interactive'],\n",
    "            label=params['label']\n",
    "        )\n",
    "    elif comp == 'Button':\n",
    "        return gr.Button(\n",
    "            value=params['value'],\n",
    "            interactive=True\n",
    "        )\n",
    "\n",
    "# @spaces.GPU(duration=120)\n",
    "def chat(img, msgs, ctx, params=None, vision_hidden_states=None):\n",
    "    default_params = {\"stream\": False, \"sampling\": False, \"num_beams\":3, \"repetition_penalty\": 1.2, \"max_new_tokens\": 1024}\n",
    "    if params is None:\n",
    "        params = default_params\n",
    "    if img is None:\n",
    "        yield \"Error, invalid image, please upload a new image\"\n",
    "    else:\n",
    "        try:\n",
    "            image = img.convert('RGB')\n",
    "            answer = model.chat(\n",
    "                image=image,\n",
    "                msgs=msgs,\n",
    "                tokenizer=tokenizer,\n",
    "                **params\n",
    "            )\n",
    "            for char in answer:\n",
    "                yield char\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "            traceback.print_exc()\n",
    "            yield ERROR_MSG\n",
    "\n",
    "\n",
    "def upload_img(image, _chatbot, _app_session):\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    _app_session['sts']=None\n",
    "    _app_session['ctx']=[]\n",
    "    _app_session['img']=image \n",
    "    _chatbot.append(('', 'Image uploaded successfully, you can talk to me now'))\n",
    "    return _chatbot, _app_session\n",
    "\n",
    "\n",
    "def respond(_chat_bot, _app_cfg):\n",
    "    _question = _chat_bot[-1][0]\n",
    "    print('<Question>:', _question)\n",
    "    if _app_cfg.get('ctx', None) is None:\n",
    "        _chat_bot[-1][1] = 'Please upload an image to start'\n",
    "        yield (_chat_bot, _app_cfg)\n",
    "    else:\n",
    "        _context = _app_cfg['ctx'].copy()\n",
    "        if _context:\n",
    "            _context.append({\"role\": \"user\", \"content\": _question})\n",
    "        else:\n",
    "            _context = [{\"role\": \"user\", \"content\": _question}]\n",
    "        params = {\n",
    "                'sampling': True,\n",
    "                'stream': True,\n",
    "                'top_p': 0.8,\n",
    "                'top_k': 100,\n",
    "                'temperature': 0.7,\n",
    "                'repetition_penalty': 1.05,\n",
    "                \"max_new_tokens\": 896 \n",
    "            }\n",
    "    \n",
    "        gen = chat(_app_cfg['img'], _context, None, params)\n",
    "        _chat_bot[-1][1] = \"\"\n",
    "        for _char in gen:\n",
    "            _chat_bot[-1][1] += _char\n",
    "            _context[-1][\"content\"] += _char\n",
    "            yield (_chat_bot, _app_cfg)\n",
    "\n",
    "\n",
    "def request(_question, _chat_bot, _app_cfg):\n",
    "    _chat_bot.append((_question, None))\n",
    "    return '', _chat_bot, _app_cfg\n",
    "\n",
    "\n",
    "def regenerate_button_clicked(_question, _chat_bot, _app_cfg):\n",
    "    if len(_chat_bot) <= 1:\n",
    "        _chat_bot.append(('Regenerate', 'No question for regeneration.'))\n",
    "        return '', _chat_bot, _app_cfg\n",
    "    elif _chat_bot[-1][0] == 'Regenerate':\n",
    "        return '', _chat_bot, _app_cfg\n",
    "    else:\n",
    "        _question = _chat_bot[-1][0]\n",
    "        _chat_bot = _chat_bot[:-1]\n",
    "        _app_cfg['ctx'] = _app_cfg['ctx'][:-2]\n",
    "    return request(_question, _chat_bot, _app_cfg)\n",
    "\n",
    "\n",
    "def clear_button_clicked(_question, _chat_bot, _app_cfg, _bt_pic):\n",
    "    _chat_bot.clear()\n",
    "    _app_cfg['sts'] = None\n",
    "    _app_cfg['ctx'] = None\n",
    "    _app_cfg['img'] = None\n",
    "    _bt_pic = None\n",
    "    return '', _chat_bot, _app_cfg, _bt_pic\n",
    "    \n",
    "css = \"\"\"\n",
    "#chatbot {\n",
    "    flex-grow: 2 !important;  /* Changed size by increasing flex-grow */\n",
    "    overflow: auto !important;\n",
    "}\n",
    "#col { height: calc(90vh - 2px - 16px) !important; }\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    gr.Markdown(\"## Your NSMQ Assistant\")\n",
    "    with gr.Tab(\"Prep\", ):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1,elem_id=\"col\"):\n",
    "                chat = gr.ChatInterface(\n",
    "                    fn=rag,\n",
    "                    chatbot=gr.Chatbot(elem_id=\"chatbot\",\n",
    "                                    render=False),\n",
    "\n",
    "                    \n",
    "                )\n",
    "           \n",
    "    with gr.Tab(\"Fast QA\"):\n",
    "        with gr.Column(elem_id=\"col\"):\n",
    "            chat = gr.ChatInterface(predict_rag, fill_height=True)\n",
    "            \n",
    "    with gr.Tab(\"Chat with images\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1, min_width=300):\n",
    "                bt_pic = gr.Image(label=\"Upload an image to start\")\n",
    "                regenerate = create_component({'value': 'Regenerate'}, comp='Button')\n",
    "                clear = create_component({'value': 'Clear'}, comp='Button')\n",
    "            with gr.Column(scale=3, min_width=500):\n",
    "                app_session = gr.State({'sts':None,'ctx':None,'img':None})\n",
    "                chat_bot = gr.Chatbot(label=f\"Chat with {model_name}\")\n",
    "                txt_message = gr.Textbox(label=\"Input text\")\n",
    "                \n",
    "                clear.click(\n",
    "                    clear_button_clicked,\n",
    "                    [txt_message, chat_bot, app_session, bt_pic],\n",
    "                    [txt_message, chat_bot, app_session, bt_pic],\n",
    "                    queue=False\n",
    "                )\n",
    "                txt_message.submit(\n",
    "                    request, \n",
    "                    [txt_message, chat_bot, app_session],\n",
    "                    [txt_message, chat_bot, app_session],\n",
    "                    queue=False\n",
    "                ).then(\n",
    "                    respond,\n",
    "                    [chat_bot, app_session],\n",
    "                    [chat_bot, app_session]\n",
    "                )\n",
    "                regenerate.click(\n",
    "                    regenerate_button_clicked,\n",
    "                    [txt_message, chat_bot, app_session],\n",
    "                    [txt_message, chat_bot, app_session],\n",
    "                    queue=False\n",
    "                ).then(\n",
    "                    respond,\n",
    "                    [chat_bot, app_session],\n",
    "                    [chat_bot, app_session]\n",
    "                )\n",
    "                bt_pic.upload(lambda: None, None, chat_bot, queue=False).then(upload_img, inputs=[bt_pic,chat_bot,app_session], outputs=[chat_bot,app_session])\n",
    "\n",
    "# launch\n",
    "demo.launch(share=True, debug=True, show_api=False, server_port=8080)\n",
    "# demo.queue()\n",
    "# demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
